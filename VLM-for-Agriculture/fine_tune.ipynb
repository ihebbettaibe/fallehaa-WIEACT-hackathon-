{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef73d4c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3fbde5244714747a5f8d450cbb8d3d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Core PyTorch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Hugging Face core\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "# Hugging Face datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Parameter-efficient fine-tuning (LoRA, adapters)\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Optional: quantization for low VRAM\n",
    "from bitsandbytes import nn as bnb\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15c32717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq, BitsAndBytesConfig\n",
    "\n",
    "\n",
    "\n",
    "# Load processor normally\n",
    "processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM-500M-Instruct\")\n",
    "\n",
    "# Update the image processor size directly\n",
    " # resizes shorter edge to 256\n",
    "\n",
    "\n",
    "# Load the VLM model without quantization for now\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"HuggingFaceTB/SmolVLM-500M-Instruct\",\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"offload\",   # folder for CPU offload if VRAM is tight\n",
    "    offload_state_dict=True,\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    " # Use half precision instead of 8-bit\n",
    "\n",
    "# Enable gradients for all parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbed4f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                          # LoRA rank (higher = stronger adaptation, more VRAM)\n",
    "    target_modules=[\"q_proj\",\"v_proj\"],  # only attention layers\n",
    "    task_type=TaskType.CAUSAL_LM,   # fine-tuning for text generation\n",
    "    lora_alpha=32,                  # scaling factor\n",
    "    lora_dropout=0.05,              # prevent overfitting\n",
    "    bias=\"none\",                     # safest for low VRAM\n",
    "    fan_in_fan_out=False,            # correct unless model uses GPT2-style Conv1D\n",
    "    init_lora_weights=True           # default init (B=0)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "016d6d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.vision_model.encoder.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.26.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.26.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.26.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.26.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.27.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.27.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.27.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.27.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.28.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.28.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.28.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.28.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.29.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.29.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.29.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.29.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.30.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.30.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.30.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.30.self_attn.v_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.31.self_attn.q_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.31.self_attn.q_proj.lora_B.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.31.self_attn.v_proj.lora_A.default.weight\n",
      "Enabled grad for: base_model.model.model.text_model.layers.31.self_attn.v_proj.lora_B.default.weight\n",
      "trainable params: 2,228,224 || all params: 509,710,528 || trainable%: 0.4372\n"
     ]
    }
   ],
   "source": [
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Explicitly enable training mode and gradients for LoRA parameters\n",
    "model.train()\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lora' in name:\n",
    "        param.requires_grad = True\n",
    "        print(f\"Enabled grad for: {name}\")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce3cabe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"llava_cddm_fixed.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3debca22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def preprocess_paths(example, image_root=\"dataset/images\"):\n",
    "    # Join root and relative path\n",
    "    rel_path = os.path.join(*example['image'].split(\"/\"))\n",
    "    image_path = os.path.abspath(os.path.join(image_root, rel_path))  # absolute path\n",
    "    image_path = os.path.normpath(image_path)  # fixes slashes for Windows\n",
    "\n",
    "    if not os.path.exists(image_path):\n",
    "        raise ValueError(f\"Image not found: {image_path}\")\n",
    "\n",
    "    # Combine conversation into single string\n",
    "    conversation_text = \"\"\n",
    "    for turn in example['conversations']:\n",
    "        role = turn['from']\n",
    "        content = turn['value'].strip()\n",
    "        if role == 'human':\n",
    "            conversation_text += f\"Human: {content}\\n\"\n",
    "        elif role == 'gpt':\n",
    "            conversation_text += f\"Assistant: {content}\\n\"\n",
    "\n",
    "    return {\n",
    "        \"image_path\": image_path,\n",
    "        \"text\": conversation_text.strip()\n",
    "    }\n",
    "dataset = dataset.map(lambda x: preprocess_paths(x, image_root=\"dataset/images\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1723c4f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\LENOVO\\\\Desktop\\\\wie act\\\\my-project\\\\dataset\\\\images\\\\Apple,Alternaria Blotch\\\\plant_74609.jpg'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]['image_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d00f6580",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "class VLMDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, processor):\n",
    "        self.dataset = hf_dataset\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        example = self.dataset[idx]\n",
    "        \n",
    "        # Make sure we have a **single image path**, not a list\n",
    "        image_path = example['image_path']\n",
    "        if isinstance(image_path, list):\n",
    "            image_path = image_path[0]\n",
    "\n",
    "        # Load image with PIL\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        text = example['text']\n",
    "\n",
    "        # Processor expects a single image here\n",
    "        inputs = self.processor(\n",
    "            images=image,\n",
    "            text=text,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Only take the first (and only) pixel_values tensor\n",
    "        pixel_values = inputs.pixel_values[0]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs.input_ids.squeeze(0),\n",
    "            \"attention_mask\": inputs.attention_mask.squeeze(0),\n",
    "            \"pixel_values\": pixel_values\n",
    "        }\n",
    "\n",
    "train_dataset = VLMDataset(dataset['train'], processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ea04c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c373a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Pad input_ids and attention_mask dynamically\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "        [b[\"input_ids\"] for b in batch],\n",
    "        batch_first=True,\n",
    "        padding_value=processor.tokenizer.pad_token_id\n",
    "    )\n",
    "    attention_mask = torch.nn.utils.rnn.pad_sequence(\n",
    "        [b[\"attention_mask\"] for b in batch],\n",
    "        batch_first=True,\n",
    "        padding_value=0\n",
    "    )\n",
    "\n",
    "    # Pad pixel values to handle different numbers of patches\n",
    "    pixel_values_list = [b[\"pixel_values\"] for b in batch]\n",
    "    max_patches = max(pv.shape[0] for pv in pixel_values_list)\n",
    "    \n",
    "    padded_pixel_values = []\n",
    "    for pv in pixel_values_list:\n",
    "        if pv.shape[0] < max_patches:\n",
    "            # Pad with zeros to match max_patches\n",
    "            padding = torch.zeros(max_patches - pv.shape[0], *pv.shape[1:], dtype=pv.dtype)\n",
    "            pv_padded = torch.cat([pv, padding], dim=0)\n",
    "        else:\n",
    "            pv_padded = pv\n",
    "        padded_pixel_values.append(pv_padded)\n",
    "    \n",
    "    pixel_values = torch.stack(padded_pixel_values)\n",
    "    \n",
    "    # Ensure tensors require gradients and are float type\n",
    "    pixel_values = pixel_values.float().requires_grad_(True)\n",
    "    input_ids = input_ids.long()\n",
    "    attention_mask = attention_mask.long()\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"labels\": input_ids  # causal LM\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26fe0b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7fc3cda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_17772\\1757513015.py:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./vlm-lora-checkpoint\",\n",
    "    per_device_train_batch_size=2,   # safe for 4GB VRAM\n",
    "    gradient_accumulation_steps=4,   # effectively bigger batch\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,                       # mixed precision if your GPU supports\n",
    "    remove_unused_columns=False      # keep pixel_values for Trainer\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    data_collator=collate_fn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ceaf0dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Shuffle and select 5000 random samples\n",
    "small_hf_dataset = dataset[\"train\"].shuffle(seed=42).select(range(5000))\n",
    "\n",
    "# Wrap in your custom dataset class\n",
    "train_dataset = VLMDataset(small_hf_dataset, processor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a833dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_17772\\742805290.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    data_collator=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d102ef23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 49279, 'bos_token_id': 1, 'pad_token_id': 2}.\n",
      "c:\\Users\\LENOVO\\llava-venv\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:186: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 40/625 2:24:10 < 36:59:33, 0.00 it/s, Epoch 0.06/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>15.305900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>13.909700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>11.832600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32db4fe1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llava-venv)",
   "language": "python",
   "name": "llava-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
